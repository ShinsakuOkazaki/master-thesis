
Among Big Data mining tools that have been developed, the most notable ones are MapReduce and Spark. MapReduce is a cluster computing framework 
which supports locality-aware scheduling, fault tolerance, and load balancing. Spark is designed to improve performance for iterative jobs keeping features of MapReduce. 

MapReduce provides a programming model where the user creates acyclic data flow graphs to pass input data through a set of operations. This 
data flow programming model is useful for many of query applications. However, MapReduce framework struggles from two of main resent data mining jobs: iterative jobs and interactive analysis. 

Iterative job is especially common in Machine Learning algorithms, such as learning a data model using Gradient Descent. In traditional MapReduce framework, each iteration can be expressed as single MapReduce job 
so that each job must reload the data from disk. This leads I/O overhead and deteriorates performance of iterative algorithms. 

Interactive analysis is also an inevitable task in modern data science. A data scientist wants to perform exploratory analysis in interactive way. 
Nevertheless, MapReduce is designed in a way more stable for ad-hoc queries, so each analysis can be single MapReduce job. 
To perform multiple analyses to explore dataset, the data needs to be written to and reloaded from disk many times. 

To overcome these limitations, Spark has been developed as a new cluster computing framework maintaining the innovative characteristics of MapReduce and improving 
its iterative and interactive jobs with in-memory data structure.

Some of the notable improvements are shown here\cite{DBLP:journals/pvldb/ShiQMJWRO15}. For aggregation operations, map output selectivity, which is the ration of the map output size to the job input size, 
can be significantly reduced by using Spark. Spark uses a map side combiner, hash-based aggregation which is more efficient than sort-based aggregation used in MapReduce. 
For iterative operations, caching the input as Resilient Distributed Datasets (RDDs) can reduce CRU and disk I/O overheads for sequence iteration. 
This RDD caching takes a significant role to improve iterative job, because it is more efficient than other low-level caching approaches such as OS buffer caches, and HDFS caching. 
These caching strategy reduces disk I/O overhead, but CPU overhead, such as parsing text to objects.
