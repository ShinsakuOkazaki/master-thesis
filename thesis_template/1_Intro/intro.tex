\chapter{Introduction}
\label{chapter:Introduction}
\thispagestyle{myheadings}

\section{Problem Description}
\label{sec:history}

Many of popular open source cluster computing frameworks for large scale data analysis, 
such as Hadoop and Spark, allow programmers to define objects in a host languages, such as Java.
The objects are then managed in RAM by the language and its runtime, Java Virtual Machine 
in the case of Java and Scala. Storing objects in memory enables machine to process iterative computation. 
One of the fundamental tasks for recent big data analysis is analysis using Machine Learning Algorithms, 
which require iterative process. As the amount of data increases, memory is required to keep many objects. 
Therefore, memory management plays a critical role in this task. 

Memory management in Java and Scala is performed by garbage collection. 
The garbage collection brings a significant advantage for programmers by removing responsibility
for planning memory management by themselves. Instead, JVM monitors the state of memory and performs garbage
collection at certain points. However, these monitoring and auto-execution of garbage collection cost additional 
computation and might consume computation resources which should be used for data processing. This can significantly decrease performance of the computation. 

In contrast, memory management in system language, such as C++, relies on programmers’ decision for when to allocate and deallocate memory. 
The functions, malloc/free consume most of the memory management. Proper implementation of system language for big data processing can be overperform the implementation in host language.
Nevertheless, implementing C++ performing proper memory management and guaranteeing security can be unproductive and complicated. 

Considering the issue of memory management, we introduce solution based on unique memory management methods implemented in Rust, ownership and borrowing.
This unique concepts in Rust secure codes and perform memory management without monitoring memory or calling functions. We introduce implementations of
machine learning algorithms in both Java and Rust to assess performances of each memory management system for iterative big data processing tasks.


\section{Memory Management in Rust}
\label{sec:history}

Each value in Rust has a variable called its owner. This owner has information about the value, such as location in memory, 
length and capacity of the value. This owner can live on the scope associated with its life time. When the owner goes out of it’s scope, 
the value will be dropped. When a value already assigned to a variable is assigned to another variable, if the value is allocated 
on heap its information is copied to the new owner and drop the old owner disabling old variable. Similar thing happens when we pass variable to parameter of function. 
After passing a variable to a parameter, all the information is copied to new owner through the parameter and old owner is no longer available. 
The new owner can only live in the function and the object will be dropped. In this case, we have no longer access to the object after the function. 
To avoid this, Rust has a concept called borrowing. We can set reference for the parameter of function and use the reference for operation within function and drop the reference, 
but not the ownership. 


\section{Spark and RDD Catching}
\label{sec:history}

Spark is one of the most used big data computing framework. Spark uses Resilient Distributed Datasets (RDDs) which implement in-memory data structures 
used to cache intermediate data across a set of nodes. This enables multiple rounds of computation on the same data, which is required for machine learning 
and graph analytics iteratively process the data. 

In RDD caching, there are different stages of caching, such as MEMOR\_YONLY and DISK\_ONLY. 
Currently, for very large data sets, we need to pay attention to garbage collection (GC) and OS page swapping overhead, 
because these could degrades execution time significantly. Therefore, DISK\_ONLY RDD caching can be better configuration in this case. 
However, writing and reading intermediate data among desk and memory could have bad effects for execution time, due to need of serialization and deserialization. 

\section{BLAS LAPACK}
\label{sec:history}

Basic Linear Algebra System (BLAS) is a linear algebra library written in Fortran. This library includes basic linear algebra functionality such as matrix addition 
and dot product. LAPACK is developed on BLAS and has advanced functionality such as LU decomposition and Singular Value Decomposition (SVD). These library optimize 
hardware use for linear algebra operation so they are hardware dependant.

Linear algebra library used for Spark is netlib-java, which is a Java wrapper library for Netlib, C API of BLAS and LAPACK. The reason why the developers addressed 
to use this package is that the BLAS and LAPACK are already bug free and implementing linear algebra library from scratch can usually buggy. 

However, the main advantage of use of BLAS and LAPACK is system optimized implementation. So if we implement original Fortran linear algebra library, 
it cannot perform as well as BLAS and LAPACK. And the performance would not be such different from one of implementation in Java or Rust.  If we want to test only memory management between Rust and Java,
 it can be enough implementation of linear algebra operation from  pure Java and Rust sacrificing the best performance taking advantage of system optimization. My concern is implementing linear algebra operation
  from scratch can be buggy and cause a lot of problems when we test on machine learning algorithms.


\section{Matrix Computation and Optimization in Apache Spark}
\label{sec:history}


Matrix operation is a fundamental part of machine learning. Apache Spark provides implementation for distributed and local matrix operation. 
To translate single-node algorithms to run on a distributed cluster, Spark addresses separating matrix operations from vector operations and run matrix operations on the cluster, 
while keeping vector operations local to the driver. 

Spark changes its behavior for matrix operations depending on the type of operations and shape of matrices. For example, Singular Value Decomposition (SVD) for a square matrix is performed in distributed cluster, 
but SVD for a tall and skinny matrix is on a driver node. This is because the matrix derived among the computation of SVD for tall and skinny matrix is usually small so that it can fit to single node.

Spark uses ARPACK to solve square SVD. ARPACK is a collection of Fortran77 designed to solve eigenvalue problems. ARPACK is based upon an algorithmic variant of the Arnoldi process called the Implicitly Restarted Arnoldi Method (IRAM). 
When the matrix A is symmetric it reduces to a variant of the Lanczos process called the Implicitly Restarted Lanczos Method (IRLM). 
ARPACK calculate matrix multiplication by performing matrix-vector multiplication. So we can distribute matrix-vector multiplies, and exploit the computational resources available in the entire cluster. 
The other method to distribute matrix operations is Spark TFOCS. Spark TFOCS supports several optimization methods.

To allow full use of hardware-specific linear algebraic operations on single node, Spark uses the BLAS (Basic Linear Algebra Systems) interface with relevant libraries for CPU and GPU acceleration. 
Native libraries can be used in Scala are ones with C BLAS interface or wrapper and called through the Java native interface implemented in Netlib-java library and wrapped by the Scala library called Breeze. 
Following is some of the implementation of BLAS.

\begin{itemize}
    \item f2jblas -  Java implementation of Fortran BLAS
    \item OpenBLAS - open source CPU-optimized C implementation of BLAS
    \item MKL - CPU-optimized C and Fortran implementation of BLAS by Intel
\end{itemize}

These have different implementation and they perform differently for the type of operation and matrices shape. 
In Sark, OpenBlas is the default method of choice. BLAS interface is made specifically for dense linear algebra. 
Then, there are few libraries that efficiently handle sparse matrix operations.


